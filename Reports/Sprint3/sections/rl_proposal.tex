\subsection{Reinforcement Learning for Boss Behavior}\label{sec:rl_proposal}

Boss behavior is trained through Reinforcement Learning (RL) to produce adaptive and challenging encounters. As mentioned in Section~\ref{sec:boss_agents}, these bosses are modeled as replicas of the player character, possessing similar movement and attack capabilities. The agents are designed to mimic the player's capabilities while exhibiting complex human-like strategies, trained in a custom Arena environment (Figure~\ref{fig:rl_training_arena}) where two agent instances fight each other.

Rather than employing pure self-play from the start, agents are first pre-trained against a heuristic opponent that moves towards/away from the agent (determined stochastically by an aggressiveness parameter $\alpha = 0.75$) and constantly attacks. After this phase, agents undergo self-play to develop sophisticated counter-strategies. The RL agents are implemented using the Unity ML-Agents Toolkit.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/rl_training_arena.png}
    \caption{Screenshot of the Arena Unity Scene used for RL training, where the boss agent learns through self-play against a copy of itself.}
    \label{fig:rl_training_arena}
\end{figure}

\subsubsection{State, Action, and Reward Design}

\paragraph{Observation Space}
The observation space consists of 18 continuous values normalized to $[-1, 1]$:
\begin{itemize}
    \item \textbf{Position and movement}: Own position $(x, y)$ normalized by arena bounds; relative position vector to enemy; own and enemy velocity vectors (normalized by max dash speed).
    \item \textbf{Health and stamina}: Own and enemy health (fraction of maximum); own stamina level for dashing.
    \item \textbf{Weapon state}: Own and enemy weapon index; own and enemy attack cooldown times.
    \item \textbf{Orientation}: Own and enemy aim angles; angle from agent to opponent.
\end{itemize}

\paragraph{Action Space}
The action space combines discrete and continuous components. \textit{Discrete actions} span three branches: movement (5 actions: stand still or move in cardinal directions), attack (2 actions: none or sword attack), and dash (2 actions: none or dash). The single \textit{continuous action} controls aim angle via a value in $[-1, 1]$, where $0$ corresponds to aiming upward ($90^{\circ}$), with positive values rotating counterclockwise and negative clockwise---this symmetric representation facilitates balanced left-right behaviors.

\paragraph{Reward Function}
The reward function encourages aggressive yet strategic behavior: victory ($r_{\mathrm{win}} = +1.0$), defeat ($r_{\mathrm{lose}} = -0.8$), landing hits ($r_{\mathrm{hit}} = +0.5$), receiving damage ($r_{\mathrm{damage}} = -0.4$). Per-timestep rewards include a proximity reward $r_{\mathrm{prox}} = +0.001 \cdot \cos(\theta_v)$ (where $\theta_v$ is the angle between velocity and direction to enemy, rewarding movement \emph{towards} opponents rather than orbiting), and a time penalty $r_{\mathrm{time}} = -0.001$ to encourage efficient combat resolution.

\subsubsection{Training Algorithm, Infrastructure, and Strategy}

\paragraph{Algorithm}
We use Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} with modified hyperparameters: batch size 2048 (increased from 1024 for stable gradients), buffer size 20480, and self-play configured with a window of 5 past policies, opponent swaps every 10,000 steps, and 50\% probability of playing against the latest versus historical snapshots.

\paragraph{Infrastructure}
Rather than duplicating Arena environments within Unity, we leverage ML-Agents' capability to use built executables as training environments. With \texttt{num\_envs: 5} and \texttt{no\_graphics: true}, five headless environment instances run in parallel, dramatically increasing throughput while reducing computational overhead. Training executes via \texttt{mlagents-learn}, interfacing with Unity executables to collect experience and update the policy using PyTorch on GPU.

\paragraph{Two-Phase Training Strategy}
Initial experiments with pure self-play revealed agents learning exploitative behaviors (e.g., retreating to corners) that appeared confused when facing humans. We adopted a two-phase approach: \textit{Phase 1} (820K steps) pre-trains against the heuristic opponent, establishing fundamental combat skills---engaging enemies, timing attacks, and basic defense. \textit{Phase 2} (4.8M steps) uses the pre-trained policy to initialize self-play, refining strategies while maintaining the aggressive foundation.

\subsubsection{Software Architecture}\label{sec:rl_methodology}

The RL system is built around several interconnected components handling agent behavior, combat mechanics, and training orchestration.

\paragraph{Agent Controller}
The \texttt{AgentController} class extends ML-Agents' \texttt{Agent} base class, responsible for collecting observations, executing actions (movement, attacks, dashing, aiming), and implementing heuristic behavior for pre-training via the \texttt{Heuristic()} method.

\paragraph{Combat Target Interface}
The player's combat system uses singletons (e.g., \texttt{PlayerHealth.Instance}), incompatible with self-play where two agents must coexist. We designed the \texttt{ICombatTarget} interface abstracting combat-relevant properties: transform/rigidbody references, health/weapon/cooldown state accessors (normalized to $[0,1]$), and aim angle/facing direction. Both \texttt{PlayerController} and \texttt{AgentController} implement this interface, allowing agents to seamlessly target either other agents (training) or the player (inference).

\paragraph{Agent-Specific Components}
To support self-play without singleton dependencies: \texttt{AgentHealth} manages health and damage; \texttt{AgentStamina} tracks stamina for dashing; \texttt{AgentWeapons} handles weapon equipping, attacks, and cooldowns; \texttt{AgentDamageSource} routes damage to appropriate targets and reports hits to the training manager.

\paragraph{Training Manager}
The \texttt{TrainingManager} orchestrates training in the Arena scene: initializes agents as each other's targets, applies per-timestep rewards, detects combat events and assigns rewards, handles episode termination and resets, and logs statistics to TensorBoard.

\paragraph{Inference Setup}
During gameplay, \texttt{AgentInferenceSetup} replaces the training manager: it sets the player as enemy target, configures arena bounds for observation normalization, handles agent death (VFX, loot, destruction), and optionally delays activation until the player enters the boss room. This separation allows the same trained prefab in both contexts.

\subsubsection{Experimental Results}\label{sec:rl_results}

Figure~\ref{fig:rl_pretraining_selfplay} shows the cumulative reward progression across both training phases. The pre-training phase (820K steps) establishes a baseline performance, followed by self-play refinement that continues improving the policy (4.8M steps).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/rl_pretraining_selfplay.png}
    \caption{Cumulative reward during the two-phase training: pre-training against heuristic opponent followed by self-play refinement.}
    \label{fig:rl_pretraining_selfplay}
\end{figure}

Figure~\ref{fig:rl_selfplay_comparison} compares the self-play phase of our two-phase approach against pure self-play training (starting from a randomly initialized policy). The pre-trained agent achieves significantly higher rewards and more stable learning, reaching an average cumulative reward of approximately $+0.5$, while pure self-play struggles to surpass $0$ even after equivalent training time.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/rl_selfplay_comparison.png}
    \caption{Comparison of self-play training performance: pure self-play (pink) versus self-play initialized from pre-trained policy (blue). The pre-trained initialization leads to substantially higher rewards and more effective combat behavior.}
    \label{fig:rl_selfplay_comparison}
\end{figure}

The qualitative difference is equally significant: agents trained with our two-phase approach actively pursue the player, engage in combat, and exhibit varied attack patterns. In contrast, pure self-play agents often display passive or exploitative behaviors that, while effective in self-play, fail to create engaging boss encounters in the actual game.
