% Generative AI Pipeline Section for Sprint 3 Final Report

\subsection{Generative AI Pipeline}
\label{subsec:genai_pipeline}

The generative AI pipeline creates dynamic narrative, music, and voice content for each chapter of the game. This section describes the final implementation and the design evolution from our initial Sprint 2 prototype to the production-ready system.

\subsubsection{Design Evolution: From Runtime to Pre-Generation}
\label{subsubsec:design_evolution}

\paragraph{Original Vision (Sprint 2)}

Our initial design in Sprint 2 was ambitious: a fully dynamic, vision-driven content generation system. The pipeline would:

\begin{enumerate}
    \item Capture a screenshot of each procedurally generated room
    \item Analyze the screenshot using a vision-language model (moondream2) to extract semantic features (environment type, atmosphere, visible objects)
    \item Feed this visual analysis to an LLM (llama2) to generate contextual narrative content
    \item Generate ambient music matching the analyzed mood
    \item Deliver all content to Unity via HTTP requests in real-time
\end{enumerate}

This approach was elegant in theory---content would dynamically adapt to the visual characteristics of each unique room layout. We implemented a FastAPI server with dedicated endpoints for vision analysis, narrative generation, and music synthesis.

\paragraph{Challenges Encountered}

Testing revealed a fundamental issue with the runtime HTTP server approach: \textbf{development iteration time}. Every code change required restarting the server, reloading all AI models into GPU memory, and running the full generation pipeline. With model loading taking 30--60 seconds and generation taking several minutes per test, the feedback loop became prohibitively slow. Simple prompt adjustments or bug fixes that should take minutes instead consumed hours of waiting.

Additionally, we encountered secondary issues:

\begin{itemize}
    \item \textbf{Vision Model Limitations}: The moondream2 vision model struggled with our pixel art aesthetic, often misidentifying dungeon elements or producing generic descriptions that didn't meaningfully differentiate between rooms.

    \item \textbf{Room-Level Granularity Mismatch}: Generating unique content for each room (potentially 5--10 per run) created inconsistency. NPCs in adjacent rooms might have contradictory dialogue, and the overall narrative felt fragmented.
\end{itemize}

\paragraph{The Pivot: Chapter-Based Pre-Generation}

We fundamentally redesigned the system based on two key insights:

\begin{enumerate}
    \item \textbf{Content should align with game structure}: Our game has three chapters, each with a distinct theme and boss. Generating content per-chapter rather than per-room creates natural narrative coherence and matches the checkpoint system.

    \item \textbf{Generation can happen offline}: Since chapter themes are defined in advance (forest $\rightarrow$ night $\rightarrow$ fire), we can generate all content before the game runs, eliminating runtime latency entirely.
\end{enumerate}

This led to the \textbf{pre-generation architecture}: a Python script (\texttt{generate.py}) runs once during development, produces all assets, and saves them to Unity's \texttt{StreamingAssets} folder. The shipped game simply loads these pre-generated files.

\paragraph{What We Removed and Why}

\begin{itemize}
    \item \textbf{Vision Analysis}: No longer needed. Our current set of visual assets is limited and not rich enough to benefit from a vision model analyzing the generated levels---chapter themes provide sufficient context. The vision model added complexity without meaningfully improving output quality.

    \item \textbf{HTTP Server}: Replaced by a simple generation script. The server approach made development iteration painfully slow---every change required a full restart and model reload cycle.

    \item \textbf{Room-by-room Generation}: Replaced by chapter-based content. Three coherent chapter narratives are more impactful than potentially dozens of disconnected room descriptions.
\end{itemize}

\paragraph{What We Added}

\begin{itemize}
    \item \textbf{Voice Generation}: With content generated offline, we could add text-to-speech synthesis without worrying about runtime latency. A narrator voice significantly enhances immersion.

    \item \textbf{Chapter-Themed Music}: Instead of trying to match music to visual features, we designed specific music prompts for each chapter's atmosphere (calm forest, tense night, dramatic finale).

    \item \textbf{Quality Control}: Pre-generation allows us to review and regenerate content that doesn't meet quality standards before shipping.
\end{itemize}

\subsubsection{Final Architecture}
\label{subsubsec:genai_architecture}

The final pipeline consists of three generation stages executed sequentially:

\begin{enumerate}
    \item \textbf{Narrative Generation}: An LLM generates dialogue lines for each chapter based on the story context and chapter theme.
    \item \textbf{Music Generation}: A text-to-music model creates ambient background music matching each chapter's atmosphere.
    \item \textbf{Voice Generation}: A text-to-speech model converts the generated dialogue into narrated voice clips.
\end{enumerate}

Figure~\ref{fig:genai_pipeline} illustrates the complete pre-generation and runtime loading architecture.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1cm and 1.8cm,
    box/.style={rectangle, draw, rounded corners, minimum width=2.2cm, minimum height=0.7cm, align=center, font=\footnotesize},
    model/.style={rectangle, draw, fill=blue!15, rounded corners, minimum width=2cm, minimum height=0.7cm, align=center, font=\footnotesize},
    file/.style={rectangle, draw, fill=green!15, minimum width=1.8cm, minimum height=0.6cm, align=center, font=\scriptsize},
    arrow/.style={->, >=stealth, thick},
    section/.style={rectangle, draw, rounded corners, fill=gray!10, inner sep=0.4cm}
]

% Pre-generation section title
\node[font=\small\bfseries] (pregentitle) {Pre-Generation (Python)};

% Config input
\node[box, below=0.6cm of pregentitle] (config) {config.json};

% Models column
\node[model, right=1.2cm of config] (ollama) {Ollama\\mistral-nemo};
\node[model, below=0.6cm of ollama] (musicgen) {MusicGen};
\node[model, below=0.6cm of musicgen] (tts) {XTTS v2};

% Output files column
\node[file, right=1.2cm of ollama] (narrative) {narrative.json};
\node[file, right=1.2cm of musicgen] (music) {music.wav};
\node[file, right=1.2cm of tts] (voice) {voice\_*.wav};

% Arrows for pre-generation
\draw[arrow] (config) -- (ollama);
\draw[arrow] (config.south) -- ++(0,-0.3) -| (musicgen.west);
\draw[arrow] (config.south) -- ++(0,-0.3) -| (tts.west);
\draw[arrow] (ollama) -- (narrative);
\draw[arrow] (musicgen) -- (music);
\draw[arrow] (tts) -- (voice);

% StreamingAssets box around output files
\node[rectangle, draw, dashed, fit=(narrative)(music)(voice), inner sep=0.25cm, label={[font=\scriptsize]below:StreamingAssets/}] (assets) {};

% Separator line
\draw[thick, gray, dashed] ([yshift=-1.2cm]config.south west) -- ++(10.5cm,0);

% Runtime section title - more space below pre-generation
\node[font=\small\bfseries, below=2.8cm of config.south west, anchor=west, xshift=-0.5cm] (runtimetitle) {Runtime (Unity C\#)};

% Runtime components
\node[box, right=1.5cm of runtimetitle] (loader) {ContentLoader};
\node[box, right=1.2cm of loader] (managers) {Managers};
\node[box, right=1.2cm of managers] (ui) {DialogueUI};

% Arrows for runtime
\draw[arrow, dashed] (assets.south) -- ++(0,-0.6) -| (loader.north);
\draw[arrow] (loader) -- (managers);
\draw[arrow] (managers) -- (ui);

\end{tikzpicture}
\caption{Generative AI pipeline architecture. The pre-generation phase runs offline using Python and GPU-accelerated models. Generated assets are saved to StreamingAssets and loaded at runtime by Unity.}
\label{fig:genai_pipeline}
\end{figure}

\paragraph{Model Selection Rationale}

Table~\ref{tab:ai_models_final} summarizes the AI models used in the final implementation. Each model was selected based on specific criteria:

\begin{table}[H]
\centering
\caption{AI models used in the generative pipeline}
\label{tab:ai_models_final}
\begin{tabular}{lccp{5cm}}
\hline
\textbf{Component} & \textbf{Model} & \textbf{Parameters} & \textbf{Selection Rationale} \\
\hline
Narrative & mistral-nemo & 12B & Fast inference, excellent instruction following for structured JSON output \\
Music & MusicGen Small~\cite{copet2023musicgen} & 300M & Best quality-to-speed ratio for ambient game music \\
Voice & XTTS v2~\cite{casanova2024xtts} & 467M & Multilingual voice cloning with natural prosody \\
\hline
\end{tabular}
\end{table}

We chose \textbf{mistral-nemo}~\cite{mistralnemo} over llama2 (used in Sprint 2) because llama2 struggled significantly with instruction following. Despite explicit prompts requesting structured JSON output, llama2 frequently produced malformed responses: missing required fields, adding unsolicited commentary, or ignoring the specified format entirely. This resulted in retry rates of approximately 40\%, wasting generation time and requiring complex parsing logic to handle edge cases. Mistral-nemo, in contrast, follows structured output instructions reliably, reducing retry rates to under 10\%. The model runs locally via Ollama~\cite{ollama}, eliminating API costs and ensuring data privacy.

\textbf{MusicGen Small} was selected after testing multiple configurations. While the Medium (1.5B) variant produces slightly higher quality output, the Small variant (300M parameters) generates acceptable quality at 4x the speed, which is important when generating music for multiple chapters. The guidance scale of 3.5 balances prompt adherence with musical coherence.

\textbf{XTTS v2} was chosen for voice generation because it produces reasonably natural multilingual speech from a single reference audio sample. The model supports voice cloning with as little as 6 seconds of reference audio, making it practical for our use case. However, XTTS v2 is not without limitations: the generated speech occasionally exhibits unnatural pacing, and the overall voice quality remains noticeably synthetic compared to professional recordings.

\paragraph{Model Selection Challenges}

Selecting appropriate generative models required extensive experimentation, as the quality gap between state-of-the-art cloud APIs and locally-runnable open-source models remains significant.

For text-to-speech, we initially tested \textbf{Parler-TTS}, which promised controllable voice generation through natural language descriptions. However, the model produced noticeable audio artifacts---particularly unwanted noises at the end of sentences and occasional ``robotic'' prosody that broke immersion. The voices often sounded unnatural, with inconsistent pacing and awkward pauses that would be distracting during gameplay. After testing multiple Parler-TTS configurations, we switched to XTTS v2, which produces more natural-sounding speech through voice cloning rather than description-based generation.

For music generation, we experimented with MusicGen's larger variants (Medium at 1.5B and Large at 3.3B parameters) but found that the quality improvement did not justify the 4--8x increase in generation time. The Small variant produces audio that is sufficiently atmospheric for background game music, where players focus primarily on gameplay rather than musical nuance.

For narrative generation, commercial APIs (OpenAI, Anthropic) produce superior text quality but introduce ongoing costs, latency dependencies, and privacy concerns. Running models locally via Ollama eliminates these issues at the cost of slightly lower output quality, which we mitigated through careful prompt engineering and retry logic.

\subsubsection{Chapter-Based Content Structure}
\label{subsubsec:chapter_content}

Content is organized by chapter rather than by room. Each of the three chapters has its own theme, narrative arc, and atmospheric music. This structure aligns with the game's checkpoint system described in Section~\ref{sec:scenario}: defeating a boss freezes that chapter's content.

The \texttt{config.json} file defines each chapter's generation parameters:

\begin{lstlisting}[language=json, basicstyle=\ttfamily\scriptsize, frame=single]
{
  "chapters": [
    {
      "name": "The Verdant Prison",
      "setting": "A sunlit forest that hides a dark truth",
      "boss": "Thornback",
      "environment": "sunlit forest with ancient trees and ruins",
      "story": "The first layer of your prison appears peaceful...",
      "music_prompt": "calm fantasy ambient, gentle flute and strings"
    },
    ...
  ]
}
\end{lstlisting}

The generation script produces the following output structure:
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single]
StreamingAssets/GeneratedContent/
  manifest.json           # Content manifest with seed and file paths
  chapters/
    chapter_0/
      narrative.json      # NPC name and dialogue lines
      music.wav           # 30-second loopable ambient track
      voice_0.wav         # Narrated first dialogue line
      voice_1.wav         # Narrated second dialogue line
      voice_2.wav         # Narrated third dialogue line
    chapter_1/
      ...
    chapter_2/
      ...
\end{lstlisting}

\subsubsection{Narrative Generation}
\label{subsubsec:narrative_gen_final}

The narrative generator creates contextual dialogue for a narrator character who introduces each chapter. Unlike the room-by-room approach in Sprint 2, narrative is now chapter-scoped to match the game's three-act structure.

The LLM receives a prompt constructed from the chapter's theme and story context:

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single]
You are a fantasy game narrator with a deep, dramatic voice.
Setting: {chapter_name}. The boss is {boss}.
Context: {progress_context}

Write exactly 3 dramatic narrator lines (full sentences, 15-25 words each)
that set the mood and hint at the danger ahead. Be atmospheric and mysterious.
Output ONLY the 3 numbered lines:
1.
2.
3.
\end{lstlisting}

The generator parses the numbered output and validates that exactly three non-empty lines are produced. A retry mechanism with seed perturbation handles occasional malformed outputs.

Example generated narrative for Chapter 0 (``The Verdant Prison''):

\begin{lstlisting}[language=json, basicstyle=\ttfamily\scriptsize, frame=single]
{
  "roomIndex": 0,
  "environment": "sunlit forest with ancient trees and overgrown ruins",
  "npc": {
    "name": "The Narrator",
    "dialogue": [
      "The forest welcomes you with golden light, but darkness lurks beneath
       the ancient canopy.",
      "Thornback, the Guardian of Growth, has twisted nature itself into a
       weapon against all who enter.",
      "Only by defeating the guardian can you proceed deeper into the realm
       and find your way home."
    ]
  }
}
\end{lstlisting}

\subsubsection{Music Generation}
\label{subsubsec:music_gen_final}

Each chapter has distinct ambient music generated using MusicGen. The music prompts are designed to evoke the emotional tone of each chapter's environment:

\begin{itemize}
    \item \textbf{Chapter 0} (Forest): ``calm fantasy ambient music, gentle flute and strings, forest atmosphere, mysterious, 90 bpm''
    \item \textbf{Chapter 1} (Night): ``tense ambient music, low strings and soft piano, night atmosphere, suspenseful and eerie, 80 bpm''
    \item \textbf{Chapter 2} (Fire): ``dramatic orchestral music, building tension, brass and percussion, climactic and intense, 110 bpm''
\end{itemize}

All prompts include the suffix ``loopable, no vocals'' to ensure the generated music is suitable for continuous background playback. The \texttt{ChapterMusicManager} component handles crossfading between chapter tracks and volume ducking during dialogue.

\subsubsection{Voice Generation}
\label{subsubsec:voice_gen}

Voice generation converts the narrative dialogue lines into spoken audio, creating an immersive narrator experience. We use XTTS v2~\cite{casanova2024xtts}, a multilingual text-to-speech model that can clone a voice from a single reference sample.

\paragraph{Why Voice Generation?}

Adding voiced narration significantly enhances immersion compared to text-only dialogue. The narrator's voice reinforces the game's atmospheric tone and provides consistent delivery across all chapters. While professional voice acting would be ideal, AI-generated voice provides a practical solution for procedurally generated content where recording every possible line is infeasible.

\paragraph{Reference Voice Selection}

We use a single 15-second reference audio sample of documentary-style narration to establish the narrator's voice characteristics. For testing purposes, we extracted a voice sample from a BBC wildlife documentary\footnote{\url{https://www.youtube.com/watch?v=ErrcHBlxOBs}}. XTTS v2 clones the prosody, pitch, and speaking style from this reference while generating the actual game dialogue. This ensures consistency across all generated voice lines while matching the dramatic, atmospheric tone we wanted for the narrator.

\paragraph{Ethical Considerations: Voice Ownership}

The ease with which modern TTS models can clone voices raises significant ethical and legal questions. With only a few seconds of audio, it is now trivial to replicate someone's voice without their consent. In our case, we used a BBC narrator's voice for development and testing, but this practice highlights broader concerns about voice property and ownership in the age of generative AI. Professional voice actors face potential displacement as their distinctive voices can be copied and used indefinitely without compensation. For a production release, original voice recordings with proper licensing would be essential to avoid intellectual property issues.

\paragraph{Generation Process}

For each dialogue line, the voice generator:
\begin{enumerate}
    \item Loads the reference audio sample
    \item Synthesizes speech for the dialogue text
    \item Saves the output as a WAV file
\end{enumerate}

Voice files are named by chapter and line index (\texttt{voice\_0.wav}, \texttt{voice\_1.wav}, etc.) and loaded by the \texttt{DialogueUI} component during gameplay.

\subsubsection{Runtime Integration}
\label{subsubsec:runtime_integration}

The Unity side of the pipeline consists of several components that load and play the pre-generated content:

\paragraph{GeneratedContentLoader}
This singleton loads the content manifest and provides asynchronous methods for loading narrative JSON, music WAV files, and voice clips. It uses Unity's \texttt{UnityWebRequest} API to load files from \texttt{StreamingAssets}, which works consistently across all platforms.

\paragraph{LLMNarrativeGenerator}
Despite its name (retained for compatibility), this component no longer calls an LLM at runtime. Instead, it loads pre-generated narratives from JSON files and caches them for retrieval by other systems.

\paragraph{ChapterMusicManager}
Manages background music playback with crossfading support. When the player enters a new chapter, this component fades out the current track and fades in the new chapter's music. During dialogue sequences, the music volume is reduced to ensure voice clarity.

\paragraph{DialogueUI}
Displays the narrator dialogue with synchronized voice playback. The component preloads voice clips for the current chapter to minimize playback latency. Voice clips are played sequentially as the player advances through dialogue lines, with an option to skip.

\paragraph{AIManagerBootstrap}
A utility component that automatically creates all AI-related managers at runtime using \texttt{RuntimeInitializeOnLoadMethod}. This ensures the managers exist before any scene-specific code attempts to access them, solving initialization order issues.

\subsubsection{Generation Performance}
\label{subsubsec:generation_performance}

Table~\ref{tab:generation_times} shows typical generation times on our development hardware (NVIDIA RTX 5070 Ti, 16GB VRAM):

\begin{table}[H]
\centering
\caption{Content generation times per chapter}
\label{tab:generation_times}
\begin{tabular}{lcc}
\hline
\textbf{Content Type} & \textbf{Time} & \textbf{Parallelizable} \\
\hline
Narrative (3 lines/chapter) & $\sim$10 seconds/chapter & Yes \\
Music (30 seconds audio) & 16--20 seconds/chapter & No (GPU-bound) \\
Voice (3 lines/chapter) & 8--15 seconds/chapter & No (GPU-bound) \\
Model loading (first run) & $\sim$50 seconds & -- \\
\hline
\textbf{Full game (3 chapters)} & \textbf{$\sim$2.5 minutes} & -- \\
\hline
\end{tabular}
\end{table}

Narrative generation is parallelized across chapters using Python's \texttt{ThreadPoolExecutor}, as the Ollama server can handle concurrent requests. Music and voice generation are executed sequentially since they compete for GPU memory.

\subsubsection{Removed Components}
\label{subsubsec:removed_components}

The following Sprint 2 components were removed in the final implementation:

\begin{itemize}
    \item \textbf{Vision Analysis (moondream2)}: Originally used to analyze room screenshots and generate semantic descriptions. Removed because our limited asset set does not provide enough visual variety to benefit from vision-based analysis---chapter themes suffice.
    \item \textbf{FastAPI HTTP Server}: Replaced by the pre-generation script. The server architecture made development iteration extremely slow, as every code change required restarting and reloading all models.
    \item \textbf{Room-by-room Generation}: Replaced by chapter-based generation to align with the game's three-act checkpoint structure.
\end{itemize}

These simplifications reduced system complexity while improving the player experience through faster load times and more consistent content quality.
