% Generative AI Pipeline Section for Sprint 3 Final Report

\subsection{Generative AI Pipeline}
\label{subsec:genai_pipeline}

The generative AI pipeline creates dynamic narrative, music, and voice content for each chapter of the game. This section describes the final implementation and the design evolution from our initial Sprint 2 prototype to the production-ready system.

\subsubsection{Design Evolution: From Runtime to Pre-Generation}
\label{subsubsec:design_evolution}

\paragraph{Original Vision (Sprint 2)}

Our initial design in Sprint 2 was ambitious: a fully dynamic, vision-driven content generation system. The pipeline would:

\begin{enumerate}
    \item Capture a screenshot of each procedurally generated room
    \item Analyze the screenshot using a vision-language model (moondream2) to extract semantic features (environment type, atmosphere, visible objects)
    \item Feed this visual analysis to an LLM (llama2) to generate contextual narrative content
    \item Generate ambient music matching the analyzed mood
    \item Deliver all content to Unity via HTTP requests in real-time
\end{enumerate}

This approach was elegant in theory---content would dynamically adapt to the visual characteristics of each unique room layout. We implemented a FastAPI server with dedicated endpoints for vision analysis, narrative generation, and music synthesis.

\paragraph{Challenges Encountered}

Testing revealed a fundamental issue with the runtime HTTP server approach: \textbf{development iteration time}. Every code change required restarting the server, reloading all AI models into GPU memory, and running the full generation pipeline. With model loading taking 30--60 seconds and generation taking several minutes per test, the feedback loop became prohibitively slow. Simple prompt adjustments or bug fixes that should take minutes instead consumed hours of waiting.

Additionally, we encountered secondary issues:

\begin{itemize}
    \item \textbf{Vision Model Limitations}: Our game assets were too limited for vision-based analysis to add meaningful value. The moondream2 model produced descriptions that were not discriminant enough from each other---with insufficient visual diversity in the procedurally generated rooms, the vision outputs were too similar to drive differentiated content. This approach could be valuable in a more complex game with richer procedural generation and greater asset variety.

    \item \textbf{Room-Level Granularity Mismatch}: Since room layouts were fully randomized, creating a structured narrative over them proved complex. The lack of semantic meaning in room placement made it difficult to generate coherent story progression tied to spatial structure.
\end{itemize}

\paragraph{The Pivot: Chapter-Based Pre-Generation}

We fundamentally redesigned the system based on two key insights:

\begin{enumerate}
    \item \textbf{Content should align with game structure}: Our game has three chapters, each with a distinct theme and boss. Generating content per-chapter rather than per-room creates natural narrative coherence and matches the checkpoint system.

    \item \textbf{Generation can happen offline}: Since chapter themes are defined in advance (forest $\rightarrow$ night $\rightarrow$ fire), we can generate all content before the game runs, eliminating runtime latency entirely.
\end{enumerate}

This led to the \textbf{pre-generation architecture}: a Python script (\texttt{generate.py}) runs once during development, produces all assets, and saves them to Unity's \texttt{StreamingAssets} folder. The shipped game simply loads these pre-generated files.

\paragraph{What We Removed and Why}

\begin{itemize}
    \item \textbf{Vision Analysis}: No longer needed. Our current set of visual assets is limited and not rich enough to benefit from a vision model analyzing the generated levels---chapter themes provide sufficient context. The vision model added complexity without meaningfully improving output quality.

    \item \textbf{HTTP Server}: Replaced by a simple generation script. The server approach made development iteration painfully slow---every change required a full restart and model reload cycle.

    \item \textbf{Room-by-room Generation}: Replaced by chapter-based content. Three coherent chapter narratives are more impactful than potentially dozens of disconnected room descriptions.
\end{itemize}

\paragraph{What We Added}

\begin{itemize}
    \item \textbf{Voice Generation}: With content generated offline, we could add text-to-speech synthesis without worrying about runtime latency. A narrator voice significantly enhances immersion.

    \item \textbf{Chapter-Themed Music}: Instead of trying to match music to visual features, we designed specific music prompts for each chapter's atmosphere (calm forest, tense night, dramatic finale).

    \item \textbf{Quality Control}: Pre-generation allows us to review and regenerate content that doesn't meet quality standards before shipping.
\end{itemize}

\subsubsection{Final Architecture}
\label{subsubsec:genai_architecture}

The final pipeline consists of three generation stages executed sequentially:

\begin{enumerate}
    \item \textbf{Narrative Generation}: An LLM generates dialogue lines for each chapter based on the story context and chapter theme.
    \item \textbf{Music Generation}: A text-to-music model creates ambient background music matching each chapter's atmosphere.
    \item \textbf{Voice Generation}: A text-to-speech model converts the generated dialogue into narrated voice clips.
\end{enumerate}

Figure~\ref{fig:genai_pipeline} illustrates the complete pre-generation and runtime loading architecture.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{figures/genai_pipeline.png}
\caption{Generative AI pipeline architecture. Pre-generation runs offline using Python and GPU-accelerated models. Assets are saved to StreamingAssets and loaded at runtime by Unity.}
\label{fig:genai_pipeline}
\end{figure}

\paragraph{Model Selection Rationale}

Table~\ref{tab:ai_models_final} summarizes the AI models used in the final implementation. Each model was selected based on specific criteria:

\begin{table}[H]
\centering
\caption{AI models used in the generative pipeline}
\label{tab:ai_models_final}
\begin{tabular}{lccp{4cm}}
\hline
\textbf{Component} & \textbf{Model} & \textbf{Params} & \textbf{Selection Rationale} \\
\hline
Narrative & mistral-nemo & 12B & Fast inference, good instruction following \\
Music & MusicGen Medium~\cite{copet2023musicgen} & 1.5B & Good quality for ambient game music \\
Voice & XTTS v2~\cite{casanova2024xtts} & 467M & Voice cloning from reference sample \\
\hline
\end{tabular}
\end{table}

We chose \textbf{mistral-nemo}~\cite{mistralnemo} over llama2 (used in Sprint 2) because llama2 struggled significantly with instruction following. Despite explicit prompts requesting structured JSON output, llama2 frequently produced malformed responses: missing required fields, adding unsolicited commentary, or ignoring the specified format entirely. This resulted in retry rates of approximately 40\%, wasting generation time and requiring complex parsing logic to handle edge cases. Mistral-nemo, in contrast, follows structured output instructions reliably, reducing retry rates to under 10\%. The model runs locally via Ollama~\cite{ollama}, eliminating API costs and ensuring data privacy.

\textbf{MusicGen Medium} (1.5B parameters) was selected after testing multiple configurations. The Small variant (300M) generates faster but with noticeably lower quality, while the Large variant (3.3B) offered marginal improvements that did not justify the increased generation time. The guidance scale of 3.5 balances prompt adherence with musical coherence.

\textbf{XTTS v2} was chosen for voice generation because it can clone a voice from a single reference audio sample. The model requires as little as 6 seconds of reference audio, making it practical for our use case. However, XTTS v2 is not without limitations: the generated speech occasionally exhibits unnatural pacing, and the overall voice quality remains noticeably synthetic compared to professional recordings.

\paragraph{Model Selection Challenges}

Selecting appropriate generative models required extensive experimentation. For text-to-speech, we initially tested \textbf{Parler-TTS}, but it produced noticeable audio artifacts and robotic prosody. XTTS v2 produces more natural speech through voice cloning. For narrative generation, commercial APIs (OpenAI, Anthropic) produce superior quality but introduce costs and latency; running models locally via Ollama eliminates these issues.

\subsubsection{Chapter-Based Content Structure}
\label{subsubsec:chapter_content}

Content is organized by chapter rather than by room. Each of the three chapters has its own theme, narrative arc, and atmospheric music. This structure aligns with the game's checkpoint system described in Section~\ref{sec:scenario}: defeating a boss freezes that chapter's content.

The \texttt{config.json} file defines each chapter's generation parameters:

\begin{lstlisting}[language=json, basicstyle=\ttfamily\scriptsize, frame=single]
{
  "chapters": [
    {
      "name": "The Verdant Prison",
      "setting": "A sunlit forest that hides a dark truth",
      "boss": "Thornback",
      "environment": "sunlit forest with ancient trees and ruins",
      "story": "The first layer of your prison appears peaceful...",
      "music_prompt": "calm fantasy ambient, gentle flute and strings"
    },
    ...
  ]
}
\end{lstlisting}

The generation script produces the following output structure:
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single]
StreamingAssets/GeneratedContent/
  manifest.json           # Content manifest with seed and file paths
  chapters/
    chapter_0/
      narrative.json      # NPC name and dialogue lines
      music.wav           # 30-second loopable ambient track
      voice_0.wav         # Narrated first dialogue line
      voice_1.wav         # Narrated second dialogue line
      voice_2.wav         # Narrated third dialogue line
    chapter_1/
      ...
    chapter_2/
      ...
\end{lstlisting}

\subsubsection{Narrative Generation}
\label{subsubsec:narrative_gen_final}

The narrative generator creates contextual dialogue for a narrator character who introduces each chapter. Unlike the room-by-room approach in Sprint 2, narrative is now chapter-scoped to match the game's three-act structure.

The LLM receives a prompt constructed from the chapter's theme and story context:

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single]
You are a fantasy game narrator with a deep, dramatic voice.
Setting: {chapter_name}. The boss is {boss}.
Context: {progress_context}

Write exactly 3 dramatic narrator lines (full sentences, 15-25 words each)
that set the mood and hint at the danger ahead. Be atmospheric and mysterious.
Output ONLY the 3 numbered lines:
1.
2.
3.
\end{lstlisting}

The generator parses the numbered output and validates that exactly three non-empty lines are produced. A retry mechanism with seed perturbation handles occasional malformed outputs.

Example generated narrative for Chapter 0 (``The Verdant Prison''):

\begin{lstlisting}[language=json, basicstyle=\ttfamily\scriptsize, frame=single]
{
  "roomIndex": 0,
  "environment": "sunlit forest with ancient trees and overgrown ruins",
  "npc": {
    "name": "The Narrator",
    "dialogue": [
      "The forest welcomes you with golden light, but darkness lurks beneath
       the ancient canopy.",
      "Thornback, the Guardian of Growth, has twisted nature itself into a
       weapon against all who enter.",
      "Only by defeating the guardian can you proceed deeper into the realm
       and find your way home."
    ]
  }
}
\end{lstlisting}

\subsubsection{Music Generation}
\label{subsubsec:music_gen_final}

Each chapter has distinct ambient music generated using MusicGen. The music prompts are designed to evoke the emotional tone of each chapter's environment:

\begin{itemize}
    \item \textbf{Chapter 0} (Forest): ``calm fantasy ambient music, gentle flute and strings, forest atmosphere, mysterious, 90 bpm''
    \item \textbf{Chapter 1} (Night): ``tense ambient music, low strings and soft piano, night atmosphere, suspenseful and eerie, 80 bpm''
    \item \textbf{Chapter 2} (Fire): ``dramatic orchestral music, building tension, brass and percussion, climactic and intense, 110 bpm''
\end{itemize}

All prompts include the suffix ``loopable, no vocals'' to ensure the generated music is suitable for continuous background playback. The \texttt{ChapterMusicManager} component handles crossfading between chapter tracks and volume ducking during dialogue.

\subsubsection{Voice Generation}
\label{subsubsec:voice_gen}

Voice generation converts narrative dialogue into spoken audio using XTTS v2. Adding voiced narration enhances immersion compared to text-only dialogue, and AI-generated voice provides a practical solution where recording every possible line is infeasible.

We use a 15-second reference sample of documentary-style narration to establish the narrator's voice. For testing, we extracted a sample from a BBC wildlife documentary\footnote{\url{https://www.youtube.com/watch?v=ErrcHBlxOBs}}. XTTS v2 clones the prosody and speaking style from this reference, ensuring consistency across all generated lines.

\paragraph{Ethical Considerations}

Modern TTS models can clone voices with only seconds of audio, raising concerns about voice ownership. For a production release, original recordings with proper licensing would be essential. Professional voice actors face potential displacement as their voices can be copied without compensation.

\subsubsection{Runtime Integration}
\label{subsubsec:runtime_integration}

The Unity side loads and plays pre-generated content through several components: \texttt{GeneratedContentLoader} handles asynchronous file loading from \texttt{StreamingAssets}, \texttt{ChapterMusicManager} manages background music with crossfading between chapters, and \texttt{DialogueUI} displays narrator dialogue with synchronized voice playback. Figure~\ref{fig:narrator_dialogue} shows the narrator dialogue UI as it appears in-game.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/rpg.png}
    \caption{In-game narrator dialogue UI. At the beginning of each chapter, the narrator introduces the environment with voice synthesis and atmospheric background music.}
    \label{fig:narrator_dialogue}
\end{figure}

\subsubsection{Generation Performance}
\label{subsubsec:generation_performance}

Table~\ref{tab:generation_times} shows typical generation times on our development hardware (NVIDIA RTX 5070 Ti, 16GB VRAM):

\begin{table}[H]
\centering
\caption{Content generation times per chapter}
\label{tab:generation_times}
\begin{tabular}{lcc}
\hline
\textbf{Content Type} & \textbf{Time} & \textbf{Parallelizable} \\
\hline
Narrative (3 lines/chapter) & $\sim$10 seconds/chapter & Yes \\
Music (30 seconds audio) & 16--20 seconds/chapter & No (GPU-bound) \\
Voice (3 lines/chapter) & 8--15 seconds/chapter & No (GPU-bound) \\
Model loading (first run) & $\sim$50 seconds & -- \\
\hline
\textbf{Full game (3 chapters)} & \textbf{$\sim$2.5 minutes} & -- \\
\hline
\end{tabular}
\end{table}

Narrative generation is parallelized across chapters using Python's \texttt{ThreadPoolExecutor}, as the Ollama server can handle concurrent requests. Music and voice generation are executed sequentially since they compete for GPU memory.
