% Generative AI Pipeline Section for Sprint 3 Final Report

\subsection{Generative AI Pipeline}
\label{subsec:genai_pipeline}

The generative AI pipeline creates dynamic narrative, music, and voice content for each chapter of the game. Rather than hand-crafting dialogue and audio assets, we leverage three specialized AI models to automatically generate coherent, chapter-themed content that enhances immersion while keeping development effort manageable.

Figure~\ref{fig:genai_pipeline_overview} illustrates the complete pipeline architecture. The system operates in two distinct phases: a \textbf{pre-generation phase} (shown in orange) where Python scripts orchestrate AI models to produce all assets offline, and a \textbf{runtime phase} (shown in pink) where Unity loads and plays the pre-generated content. This separation eliminates runtime latency entirely---players experience instant content delivery while the computationally expensive generation happens once during development.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{figures/genai_pipeline_diagram.png}
\caption{Generative AI pipeline architecture. Configuration (blue) defines chapter themes and prompts. The pre-generation phase (orange) runs three AI models: narrative generation feeds into voice synthesis, while music generation runs in parallel. Outputs are stored in StreamingAssets (green) and loaded at runtime by Unity (pink).}
\label{fig:genai_pipeline_overview}
\end{figure}

The pipeline processes content at the \textbf{chapter level} rather than per-room, aligning with the game's three-act structure. For each chapter, the system generates: (1) three narrator dialogue lines via an LLM, (2) a 30-second ambient music track via a text-to-music model, and (3) voice clips for each dialogue line via text-to-speech.

\subsubsection{Design Evolution: From Runtime to Pre-Generation}
\label{subsubsec:design_evolution}

The primary goal of this pipeline is to \textbf{demonstrate how generative AI can enhance game content}---producing narrative, music, and voice that would traditionally require manual creation. Whether generation happens at runtime or offline is an MLOps concern, not the core contribution.

\paragraph{Original Vision (Sprint 2)}
Our initial design used a vision-driven runtime system: capture room screenshots, analyze them with a vision-language model (moondream2), generate contextual narrative via LLM (llama2), create matching music, and deliver everything to Unity via HTTP in real-time.

\paragraph{Challenges Encountered}
Testing revealed critical issues: (1) \textbf{Development iteration time}---every code change required restarting the server and reloading models (30--60s), making the feedback loop prohibitively slow; (2) \textbf{Vision model limitations}---our game assets lacked sufficient visual diversity for meaningful differentiation; (3) \textbf{Room-level granularity mismatch}---randomized room layouts made coherent narrative progression difficult.

\paragraph{The Pivot}
We redesigned around two insights: content should align with game structure (three chapters with distinct themes), and generation can happen offline since themes are predefined. This led to the pre-generation architecture in Figure~\ref{fig:genai_pipeline_overview}: a Python script runs once during development, producing all assets saved to Unity's \texttt{StreamingAssets} folder.

This change does not diminish the generative AI contribution---the same models produce the same quality content. It also enabled adding \textbf{voice generation} (no runtime latency concerns) and \textbf{quality control} (review and regenerate before shipping).

\subsubsection{Model Selection}
\label{subsubsec:model_selection}

Table~\ref{tab:ai_models_final} summarizes the AI models used. All run locally via Ollama~\cite{ollama}, eliminating API costs and ensuring data privacy.

\begin{table}[H]
\centering
\caption{AI models used in the generative pipeline}
\label{tab:ai_models_final}
\begin{tabular}{lccp{4cm}}
\hline
\textbf{Component} & \textbf{Model} & \textbf{Params} & \textbf{Selection Rationale} \\
\hline
Narrative & mistral-nemo~\cite{mistralnemo} & 12B & Fast inference, good instruction following \\
Music & MusicGen Small~\cite{copet2023musicgen} & 300M & Fast generation, acceptable quality \\
Voice & XTTS v2~\cite{casanova2024xtts} & 467M & Voice cloning from reference sample \\
\hline
\end{tabular}
\end{table}

We chose \textbf{mistral-nemo} over llama2 (Sprint 2) due to superior instruction following. \textbf{MusicGen Small} provides fast generation with acceptable quality for ambient game music. \textbf{XTTS v2} enables voice cloning from 6 seconds of reference audio; we tested Parler-TTS first but it produced robotic prosody.

\subsubsection{Content Generation}
\label{subsubsec:chapter_content}

Content is organized by chapter, each with its own theme, narrative arc, and atmospheric music. A configuration file defines generation parameters per chapter (name, setting, boss, music prompt). See Appendix~\ref{app:genai_details} for prompt templates and example outputs.

\paragraph{Narrative Generation}
The LLM receives a prompt with chapter theme and story context, generating exactly 3 dramatic narrator lines (15--25 words each). A retry mechanism handles occasional malformed outputs.

\paragraph{Music Generation}
Each chapter has distinct ambient music generated using MusicGen with prompts designed to evoke the emotional tone. All prompts include ``loopable, no vocals'' for continuous background playback.

\begin{itemize}
    \item \textbf{Chapter 0} (Forest): calm fantasy ambient, gentle flute and strings, 90 bpm \\
    \textattachfile[color=0 0 1, icon=Speaker, description={Chapter 0 - Forest ambient music}]{audio/chapter0_music.wav}{\textcolor{blue}{Play/Download Audio}}
    \item \textbf{Chapter 1} (Night): tense ambient, low strings and soft piano, 80 bpm \\
    \textattachfile[color=0 0 1, icon=Speaker, description={Chapter 1 - Night ambient music}]{audio/chapter1_music.wav}{\textcolor{blue}{Play/Download Audio}}
    \item \textbf{Chapter 2} (Fire): dramatic orchestral, brass and percussion, 110 bpm \\
    \textattachfile[color=0 0 1, icon=Speaker, description={Chapter 2 - Fire ambient music}]{audio/chapter2_music.wav}{\textcolor{blue}{Play/Download Audio}}
\end{itemize}

\paragraph{Voice Generation}
XTTS v2 converts narrative dialogue into spoken audio, cloning prosody from a 15-second documentary-style reference sample\footnote{Reference: \url{https://www.youtube.com/watch?v=ErrcHBlxOBs}}. Sample outputs:
\textbf{Ch.0}: \textattachfile[color=0 0 1, icon=Speaker, description={Chapter 0 - Voice}]{audio/chapter0_voice.wav}{\textcolor{blue}{Play}}
\textbf{Ch.1}: \textattachfile[color=0 0 1, icon=Speaker, description={Chapter 1 - Voice}]{audio/chapter1_voice.wav}{\textcolor{blue}{Play}}
\textbf{Ch.2}: \textattachfile[color=0 0 1, icon=Speaker, description={Chapter 2 - Voice}]{audio/chapter2_voice.wav}{\textcolor{blue}{Play}}

\paragraph{Ethical Considerations}
Modern TTS models can clone voices with seconds of audio, raising concerns about voice ownership. For production, original recordings with proper licensing would be essential.

\subsubsection{Runtime Integration and Performance}
\label{subsubsec:runtime_integration}

Unity loads pre-generated content via \texttt{GeneratedContentLoader}, with \texttt{ChapterMusicManager} handling crossfading and \texttt{DialogueUI} displaying narrator dialogue with synchronized voice playback. Figure~\ref{fig:narrator_dialogue} shows the in-game UI.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/rpg.png}
    \caption{In-game narrator dialogue UI with voice synthesis and atmospheric background music.}
    \label{fig:narrator_dialogue}
\end{figure}

Table~\ref{tab:generation_times} shows generation times on our development hardware (NVIDIA RTX 5070 Ti, 16GB VRAM). Full game content generates in $\sim$2.5 minutes.

\begin{table}[H]
\centering
\caption{Content generation times per chapter}
\label{tab:generation_times}
\begin{tabular}{lcc}
\hline
\textbf{Content Type} & \textbf{Time} & \textbf{Parallelizable} \\
\hline
Narrative (3 lines) & $\sim$10s & Yes \\
Music (30s audio) & 30--35s & No (GPU-bound) \\
Voice (3 lines) & 8--15s & No (GPU-bound) \\
\hline
\textbf{Full game (3 chapters)} & \textbf{$\sim$2.5 min} & -- \\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion and Future Perspectives}

This generative AI pipeline serves as a \textbf{proof of concept} demonstrating how modern AI models can enhance game content creation. By simply modifying a configuration file---or potentially prompting the player directly---games could offer fully customized experiences limited only by imagination: personalized narratives, unique soundtracks, and character voices tailored to each player's preferences.

From my perspective, generative AI in games presents two promising directions. First, \textbf{single-asset generation} offers immediate practical value: an indie developer could generate voice acting or music without the heavy production costs of professional studios, democratizing game development. Second, and more ambitiously, I believe there is an opportunity to create \textbf{a new type of game engine} specifically designed around generative AI---where every aspect of the game (narrative, visuals, audio, mechanics) could be dynamically generated and controlled by the player in real-time, fundamentally changing what interactive experiences can be.
